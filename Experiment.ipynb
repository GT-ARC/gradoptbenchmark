{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python basic imports\n",
    "import math\n",
    "import multiprocessing\n",
    "#3rd party imports (from packages, the environment)\n",
    "import numpy as np\n",
    "#custom (local) imports\n",
    "import experiment.config as config\n",
    "from util.database import Database\n",
    "from util.logging import setupLogging, shutdownLogging\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from util.worker import worker\n",
    "from experiment.optimizers import MonteCarloSampling1M\n",
    "import experiment.optimizers as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Logging\n",
      "Evaluation ac7edd350532958d8d785b6af611fe427604846151319cc8a946360e087e35fb\n",
      "could not find json file to load from: results.json\n"
     ]
    }
   ],
   "source": [
    "#Experimental Setup\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = setupLogging()\n",
    "    logger.info(\"Evaluation \"+str(config.EVALUATION_HASH))\n",
    "    db = Database()\n",
    "    db.loadFromJson(config.DATABASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No MonteCarloSampling exists, that can happen if you use an incomplete result.json file. Don't panic, just run the Benchmark further to fix this issue\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    maxScoresDict = dict()\n",
    "    bestAvgScoresDict = Database()\n",
    "    sumOfScoresDict = dict()\n",
    "    noOfScoresDict = dict()\n",
    "    meanOfScoresDict = dict()\n",
    "    targetValuesDict = dict()\n",
    "    \n",
    "    for optimizerId,optimizerValues in db.core.items():\n",
    "        for problemId,problemValues in optimizerValues.items():\n",
    "            for k,values in problemValues.items():\n",
    "                #logger.info(values.items())\n",
    "                score = values['bestscore']\n",
    "\n",
    "                \n",
    "                if problemId in maxScoresDict:\n",
    "                    if score > maxScoresDict[problemId]:\n",
    "                        maxScoresDict[problemId] = score\n",
    "                else:\n",
    "                    maxScoresDict[problemId] = score\n",
    "                \n",
    "                #we calculate the mean only from the MonteCarloSampling1M optimizer.\n",
    "                if ( optimizerId == MonteCarloSampling1M.__name__ ):\n",
    "                    allScores = values['scores']\n",
    "                    noOfScores = np.shape(allScores)[0]\n",
    "                    sumOfScores = np.sum(allScores)\n",
    "                    \n",
    "                    #because we do this for each k:\n",
    "                    if problemId in sumOfScoresDict:\n",
    "                        sumOfScores += sumOfScoresDict[problemId]\n",
    "                        noOfScores += noOfScoresDict[problemId]\n",
    "                    \n",
    "                    sumOfScoresDict[problemId] = sumOfScores\n",
    "                    noOfScoresDict[problemId] = noOfScores\n",
    "\n",
    "    #note: we only go over problems where we can calculate the mean values of f(x), e.g. where the monte carlo sampling exists.                \n",
    "    if ( len(sumOfScoresDict.keys()) == 0):\n",
    "        logger.info(\"No MonteCarloSampling exists, that can happen if you use an incomplete result.json file. Don't panic, just run the Benchmark further to fix this issue\")\n",
    "    for problemId in sumOfScoresDict.keys():\n",
    "        mx = maxScoresDict[problemId]\n",
    "        sm = sumOfScoresDict[problemId]\n",
    "        mean = sm / noOfScoresDict[problemId]\n",
    "        \n",
    "        def target(t):\n",
    "            return mx - ( ( mx - mean ) * (1-t) ) \n",
    "        targetValues = np.array([ target(t) for t in config.TARGETS ])\n",
    "        \n",
    "        logger.info(\"mx: \"+str(mx))\n",
    "        logger.info(\"mean: \"+str(mean))    \n",
    "        logger.info(\"target values of \"+str(problemId)+\" are: \"+str(targetValues))\n",
    "        targetValuesDict[problemId] = targetValues\n",
    "    \n",
    "    \n",
    "    refinedResults = Database()\n",
    "    for optimizerId,optimizerValues in db.core.items():\n",
    "        #dont take the dummy optimizer MonteCarloSampling1M into the final evaluation\n",
    "        if ( optimizerId == MonteCarloSampling1M.__name__ ):\n",
    "            continue\n",
    "        for problemId in sumOfScoresDict.keys():\n",
    "            if not(problemId in optimizerValues.keys()):\n",
    "                continue\n",
    "            problemValues = optimizerValues[problemId]\n",
    "            indexesWhereTargetValuesWhereExceeded = np.zeros((config.K,len(config.TARGETS)))\n",
    "            logger.info(optimizerId+\" on \"+problemId)\n",
    "            valueCounter = 0#only for debugging / validation\n",
    "            completionCounter = 0 #count how many runs actually have config.MAX_EVALUATIONS values in them\n",
    "            for k,values in problemValues.items():\n",
    "                allScores = values['scores']\n",
    "                valueCounter += np.shape(allScores)[0]\n",
    "                completionCounter += int( np.shape(allScores)[0] >= config.MAX_EVALUATIONS )\n",
    "                #check, at which index the values are above the target values\n",
    "                mask = np.expand_dims( np.array(allScores), axis=-1) > np.expand_dims(targetValuesDict[problemId],axis=-1).T\n",
    "                indexWhereTargetValuesAreHit = np.argmax(mask, axis=0)\n",
    "                #if the algorithm never reached the target value, it is set to the maximum number of iterations:\n",
    "                indexWhereTargetValuesAreHit[ np.diag( mask[indexWhereTargetValuesAreHit] ) == False ] = config.MAX_EVALUATIONS-1#since we count from index 0\n",
    "                indexesWhereTargetValuesWhereExceeded[int(k),:] = indexWhereTargetValuesAreHit\n",
    "            \n",
    "            meanValue = np.mean(indexesWhereTargetValuesWhereExceeded, axis=0) + 1 #(since we count from index 0)\n",
    "            stdValue = np.std(indexesWhereTargetValuesWhereExceeded, axis=0)\n",
    "            \n",
    "            refinedResults.store(optimizerId, problemId, 'mean', meanValue)\n",
    "            refinedResults.store(optimizerId, problemId, 'std', stdValue)   \n",
    "            logger.info(\"mean: \"+str( meanValue ) )\n",
    "            logger.info(\"std: \"+str( stdValue ) )\n",
    "            logger.info(\"completed runs: \"+str(100.0*completionCounter/config.K)+\"%\")\n",
    "            logger.info(\"completed function calls: \"+str(100.0*valueCounter/(config.K*config.MAX_EVALUATIONS))+\"%\")\n",
    "            \n",
    "    \n",
    "    #Determine, which optimizer was the best on on a problem in order to plot it thick\n",
    "    for optimizerId,optimizerValues in refinedResults.core.items():\n",
    "        for problemId,problemValues in optimizerValues.items():\n",
    "            meanValues = problemValues['mean'].copy()\n",
    "            if  bestAvgScoresDict.exists(problemId):\n",
    "                oldValues = bestAvgScoresDict.get(problemId, 'values')\n",
    "                oldBestOptimizerIds = bestAvgScoresDict.get(problemId, 'optimizerId')\n",
    "                mask = ( meanValues < oldValues )\n",
    "                maskEqual = ( meanValues == oldValues )\n",
    "                oldValues[mask] = meanValues[mask]\n",
    "                \n",
    "                for i in range(np.shape(meanValues)[0]):\n",
    "                    if mask[i]: oldBestOptimizerIds[i] = list([optimizerId])\n",
    "                    if maskEqual[i]: oldBestOptimizerIds[i].append(optimizerId)\n",
    "\n",
    "                bestAvgScoresDict.store(problemId, 'values', oldValues)\n",
    "                bestAvgScoresDict.store(problemId, 'optimizerId', oldBestOptimizerIds)\n",
    "            else:\n",
    "                bestAvgScoresDict.store(problemId, 'values', meanValues)\n",
    "                bestAvgScoresDict.store(problemId, 'optimizerId',[list([optimizerId]) for i in range(np.shape(meanValues)[0])])\n",
    "\n",
    "\n",
    "def applyFilter(currentId, filterDict):\n",
    "    if filterDict == None:\n",
    "        return False\n",
    "    if currentId in filterDict:\n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoMPGHD', 'BreastCancerHD', 'SlumpHD', 'YachtHD', 'HousingHD', 'AutoMPG', 'BreastCancer', 'Slump', 'Yacht', 'Housing', 'HolderTable', 'Rosenbrock', 'Sphere', 'SphereHighDim', 'LinearSlope', 'DebN1', 'BraninHoo', 'Himmelblau', 'Styblinski', 'LevyN13', 'MishraN2', 'GriewankN4', 'TestProblem', 'TestProblem2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>    <tr>    <td>90% Target</td></table><table>    <tr>    <td>95% Target</td></table><table>    <tr>    <td>99% Target</td></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Render as html table\n",
    "def createHTMLResultTable(tableString, index, problemFilter = None, optimizerFilter = None):\n",
    "    tableString += '<table>'\n",
    "    tableString += '    <tr>'\n",
    "    tableString += '    <td>'+str(int(config.TARGETS[index]*100))+'% Target</td>'\n",
    "\n",
    "    for problemId in maxScoresDict.keys():\n",
    "        if ( applyFilter(problemId, problemFilter ) ): continue\n",
    "        tableString += '<td>'+problemId+'</td>'\n",
    "\n",
    "    for optimizerId,optimizerValues in refinedResults.core.items():\n",
    "        if ( applyFilter(optimizerId, optimizerFilter ) ): continue\n",
    "        tableString += '<tr>'\n",
    "        tableString += '<td>'+optimizerId+'</td>'\n",
    "\n",
    "        for problemId,problemValues in optimizerValues.items():\n",
    "            if ( applyFilter(problemId, problemFilter ) ): continue\n",
    "\n",
    "            isBestInCategory = ( optimizerId in (bestAvgScoresDict.get(problemId, 'optimizerId')[index]) )\n",
    "            bestStartTag = ''\n",
    "            bestEndTag = ''\n",
    "            if ( isBestInCategory ):\n",
    "                bestStartTag = '<b>'\n",
    "                bestEndTag = '</b>'\n",
    "                \n",
    "            tableString += '<td>' + bestStartTag + str(problemValues['mean'][index]) + bestEndTag + '(+-' +str((\"%.2f\" % problemValues['std'][index]))+')' + '</td>'\n",
    "    tableString += '</table>'\n",
    "    return tableString\n",
    "\n",
    "def renderHTMLTables(problemFilter = None, optimizerFilter = None):\n",
    "    tableString = \"\"\n",
    "    #Create one table per target setting\n",
    "    for i in range(len(config.TARGETS)):\n",
    "        tableString = createHTMLResultTable(tableString, i, problemFilter, optimizerFilter)\n",
    "    display(HTML(tableString))\n",
    "    \n",
    "allProblems = [cls.__name__ for cls in config.problems.ALL.values()]\n",
    "#optimizerFilter = [optimizers.PureRandomSearch.__name__, optimizers.BayesianOptimization.__name__, optimizers.HierarchicalOptimisticOptimization.__name__, optimizers.GradOpt.__name__]\n",
    "print(allProblems)\n",
    "renderHTMLTables(problemFilter = allProblems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shutdown logger, bye bye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    shutdownLogging()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
